{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchita-puri/Automated-Detection-of-Parkinson-s-Disease-Using-LDA-NN-GA/blob/main/DB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khGwmsOKvZaN",
        "outputId": "529c88a1-4814-44c0-f638-9c7d62079474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn-genetic-opt\n",
            "  Downloading sklearn_genetic_opt-0.8.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-genetic-opt) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.61.1 in /usr/local/lib/python3.7/dist-packages (from sklearn-genetic-opt) (4.63.0)\n",
            "Collecting deap>=1.3.1\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 71 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 92 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 102 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 112 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 122 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 133 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 143 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 153 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 160 kB 20.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from sklearn-genetic-opt) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->sklearn-genetic-opt) (3.1.0)\n",
            "Installing collected packages: deap, sklearn-genetic-opt\n",
            "Successfully installed deap-1.3.1 sklearn-genetic-opt-0.8.1\n"
          ]
        }
      ],
      "source": [
        "pip install sklearn-genetic-opt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkgg5ANJYQ5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tMsYVwOWs06B",
        "outputId": "06465646-7d98-44d1-8960-5e8203038c08"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-12220540dc92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevolutionary_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvolutionaryAlgorithmSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evolutionary_search'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn_genetic import GASearchCV\n",
        "from sklearn_genetic.space import Continuous, Categorical, Integer\n",
        "from sklearn_genetic.plots import plot_fitness_evolution, plot_search_space\n",
        "\n",
        "from scipy.stats import zscore\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68AhquaMtG1S",
        "outputId": "18f38902-e7c3-411f-e2fa-4cab5bb124d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
            "X contains 26 input features only and there is no output label (1040, 26)\n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "[[1.809000e+00 1.485100e-04 6.800000e-01 8.430000e-01 2.040000e+00\n",
            "  7.881000e+00 7.820000e-01 2.690000e+00 4.543000e+00 1.107300e+01\n",
            "  8.069000e+00 9.255540e-01 9.748100e-02 1.347200e+01 1.192600e+02\n",
            "  1.216300e+02 8.028000e+00 1.081440e+02 1.375460e+02 6.200000e+01\n",
            "  6.000000e+01 8.211245e-03 5.658130e-04 1.818200e+01 1.000000e+00\n",
            "  3.387000e+00]]\n"
          ]
        }
      ],
      "source": [
        "#Importing the Training Data \n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)\n",
        "print(X[[1039]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNefmKwTzgKs"
      },
      "source": [
        "**Below code extracts vowel 'u' phonations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL1nJWw_1JsS",
        "outputId": "e4d2d001-6808-4ab5-d56e-6ad8190c9368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "indices = 2 #if you change this to 1 or 2, also change the increment to 27 and 28, respectively\n",
        "increment=28\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c90mszry_rn"
      },
      "source": [
        "**Below code extracts vowel 'o' phonations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67QG5XO-qPza",
        "outputId": "b85b7f9e-33bf-4022-e40e-30fa56c2ca40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement EvolutionaryAlgorithmSearchCV (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for EvolutionaryAlgorithmSearchCV\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install EvolutionaryAlgorithmSearchCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNy3f2HJ1X_N",
        "outputId": "1a2e28e9-8720-4590-9f25-ea0d1a09293d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "indices = 1 #if you change this to 1 or 2, also change the increment to 27 and 28, respectively\n",
        "increment=27\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KmJ6uvk2HK5"
      },
      "source": [
        "**Below code extracts vowel 'a' phonations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hh0Qus-16b6",
        "outputId": "49439b8f-31d2-4207-9be2-04d8d17e83f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "indices = 1 #if you change this to 1 or 2, also change the increment to 27 and 28, respectively\n",
        "increment=27\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylykt0h10RAk"
      },
      "source": [
        "**Below Code is for Genetic Algorithm Code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNZehctj0Wp1",
        "outputId": "ec0372e5-7462-48c5-af00-fb7e31e7b367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size:  2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import itertools\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "X_Origionl = X\n",
        "y_Origionl = Y\n",
        "\n",
        "x = range(1,101)\n",
        "lda = LDA(n_components=1)\n",
        "X = lda.fit_transform(X_Origionl, y_Origionl)\n",
        "y = y_Origionl\n",
        "paramgrid = {\"hidden_layer_sizes\": list(itertools.combinations(x,2)),\n",
        "             \"solver\" :['lbfgs']\n",
        "            \n",
        "             }\n",
        "print(\"Size: \", len(paramgrid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "km-2htoy0hEf",
        "outputId": "45a5139c-e77b-476d-c06e-662b39841803"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0b12b23a1981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#pool = Pool(4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     cv = EvolutionaryAlgorithmSearchCV(estimator=MLPClassifier(),\n\u001b[0m\u001b[1;32m      5\u001b[0m                                        \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparamgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                        \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EvolutionaryAlgorithmSearchCV' is not defined"
          ]
        }
      ],
      "source": [
        "time_a = time.time()\n",
        "if __name__==\"__main__\":\n",
        "    #pool = Pool(4)\n",
        "    cv = EvolutionaryAlgorithmSearchCV(estimator=MLPClassifier(),\n",
        "                                       params=paramgrid,\n",
        "                                       scoring=\"accuracy\",\n",
        "                                       cv=KFold(n_splits=40),\n",
        "                                       verbose=True,\n",
        "                                       population_size=15,\n",
        "                                       gene_mutation_prob=0.10,\n",
        "                                       tournament_size=3,\n",
        "                                       generations_number=10)\n",
        "                                       #pmap = pool.map)\n",
        "    %time cv.fit(X, y)\n",
        "time_b = time.time()\n",
        "print(\"Time Elapsed =\", time_b-time_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3r_AfMF5tL6"
      },
      "source": [
        "**Time complexity analysis with grid search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "3ZrqbkG_6hPC",
        "outputId": "f8b31f8e-0b9a-4f1e-8c0f-83b75cfc121c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Acc ==================================== 90.0\n",
            "Nodes1 = 1 Nodes2= 4\n",
            "Best Acc ==================================== 92.5\n",
            "Nodes1 = 1 Nodes2= 7\n",
            "Best Acc ==================================== 95.0\n",
            "Nodes1 = 1 Nodes2= 65\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-48fd09340e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \"\"\"\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             self._fit_lbfgs(\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             )\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             },\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         )\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_func_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"log_loss\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_activation_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"logistic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mloss_func_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary_log_loss\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOSS_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_func_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Add L2 regularization term to loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_base.py\u001b[0m in \u001b[0;36mbinary_log_loss\u001b[0;34m(y_true, y_prob)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mare\u001b[0m \u001b[0mcorrectly\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     return (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finfo_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_new = lda.fit_transform(X, Y)\n",
        "\n",
        "Best_Acc =0\n",
        "\n",
        "time_a = time.time()\n",
        "Nodes = range(1,101)\n",
        "for nodes1 in Nodes:\n",
        "    for nodes2 in Nodes:\n",
        "        i=0\n",
        "        Net_Acc=0\n",
        "        while i<=39:\n",
        "          #########Setting Train Test Parts for Each Subject\n",
        "            X_test = X_new[[i]]\n",
        "            X_train = numpy.delete(X_new, i, 0)\n",
        "            Y_train = numpy.delete(Y, i, 0)\n",
        "            Y_test = Y[[i]]\n",
        "\n",
        "\n",
        "            ##############Models Part##############################################\n",
        "            #Model Fiting\n",
        "            # checkpoint\n",
        "            model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(nodes1, nodes2,),random_state=1)\n",
        "            model.fit(X_train, Y_train)\n",
        "            Y_pred = model.predict(X_test)\n",
        "            # evaluate the model\n",
        "            scores = accuracy_score(Y_test, Y_pred)\n",
        "            #print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "            if scores==1:\n",
        "                Net_Acc = Net_Acc+1\n",
        "\n",
        "            i = i+1\n",
        "        Acc = (Net_Acc/40)*100\n",
        "        \n",
        "        if (Acc>Best_Acc):\n",
        "            Best_Acc = Acc\n",
        "            print(\"Best Acc ====================================\", Best_Acc)\n",
        "            print(\"Nodes1 =\", nodes1, \"Nodes2=\", nodes2)\n",
        "time_b = time.time()\n",
        "print(\"Time Elapsed =\", time_b-time_a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFcbDr5U6ybM"
      },
      "source": [
        "**Regenerating Results of Table III of the Paper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSRlgVll6z1Q",
        "outputId": "3ea7f1cb-959d-482a-86d1-bf3dbfccd1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
            "X contains 26 input features only and there is no output label (1040, 26)\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Importing the Train Data \n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcwBBQqH7G1c",
        "outputId": "6018853a-271a-4c61-bcb5-43e23ef269a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n",
            "[   2   28   54   80  106  132  158  184  210  236  262  288  314  340\n",
            "  366  392  418  444  470  496  522  548  574  600  626  652  678  704\n",
            "  730  756  782  808  834  860  886  912  938  964  990 1016]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Extracting Vowel \"u\" Phonations.\n",
        "i=1\n",
        "indices = 2 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
        "increment=28\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(indices)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI-Vs4LO7S8a",
        "outputId": "afad307c-0f11-4816-8502-299a67629c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc ==================================== 95.0\n",
            "Sensitivity = 95.0\n",
            "Specificity = 95.0\n",
            "Mathew Correlation Coefficients MCC= 0.9\n"
          ]
        }
      ],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_new = lda.fit_transform(X, Y)\n",
        "\n",
        "Best_Acc =0\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "\n",
        "i=0\n",
        "Net_Acc=0\n",
        "while i<=39:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test = X_new[[i]]\n",
        "    X_train = numpy.delete(X_new, i, 0)\n",
        "    Y_train = numpy.delete(Y, i, 0)\n",
        "    Y_test = Y[[i]]\n",
        "\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
        "    model.fit(X_train, Y_train)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    # evaluate the model\n",
        "    scores = accuracy_score(Y_test, Y_pred)\n",
        "    \n",
        "    if i<=19:\n",
        "        if scores==1:\n",
        "            TP = TP+1 #This is for evaluating sensitivity\n",
        "    if i>19:\n",
        "       if scores==1:\n",
        "            TN = TN+1  #This is for evaluating specificity\n",
        "    \n",
        "\n",
        "    if scores==1:\n",
        "        Net_Acc = Net_Acc+1 #This is for evaluating overall accuracy\n",
        "\n",
        "    i = i+1\n",
        "Acc = (Net_Acc/40)*100\n",
        "FN=20-TP\n",
        "FP=20-TN\n",
        "\n",
        "Best_Acc = Acc\n",
        "print(\"Acc ====================================\", Best_Acc)\n",
        "print(\"Sensitivity =\", (TP/(20))*100)\n",
        "print(\"Specificity =\", (TN/(20))*100)\n",
        "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cw9_-kh7d40"
      },
      "source": [
        "**Regenerating Results of Table IV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AROEO6tL7fDG",
        "outputId": "05b19375-8d89-4fd9-9498-1aeb0ced227f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
            "X contains 26 input features only and there is no output label (1040, 26)\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Importing the data of Training Database\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS7jiOpu7hkv",
        "outputId": "2f74e2ec-8d97-48b9-886a-5aae1710207e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n",
            "[   1   27   53   79  105  131  157  183  209  235  261  287  313  339\n",
            "  365  391  417  443  469  495  521  547  573  599  625  651  677  703\n",
            "  729  755  781  807  833  859  885  911  937  963  989 1015]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Extracting Vowel \"o\" Phonations.\n",
        "i=1\n",
        "indices = 1 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
        "increment=27\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(indices)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj62diXG7qfD",
        "outputId": "20f28a44-4af1-44bc-b932-1e7654d6b84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc ==================================== 92.5\n",
            "Sensitivity = 95.0\n",
            "Specificity = 90.0\n",
            "Mathew Correlation Coefficients MCC= 0.8510644963469901\n"
          ]
        }
      ],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_new = lda.fit_transform(X, Y)\n",
        "\n",
        "Best_Acc =0\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "\n",
        "i=0\n",
        "Net_Acc=0\n",
        "while i<=39:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test = X_new[[i]]\n",
        "    X_train = numpy.delete(X_new, i, 0)\n",
        "    Y_train = numpy.delete(Y, i, 0)\n",
        "    Y_test = Y[[i]]\n",
        "\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
        "    model.fit(X_train, Y_train)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    # evaluate the model\n",
        "    scores = accuracy_score(Y_test, Y_pred)\n",
        "    \n",
        "    if i<=19:\n",
        "        if scores==1:\n",
        "            TP = TP+1\n",
        "    if i>19:\n",
        "        if scores==1:\n",
        "            TN = TN+1\n",
        "    \n",
        "\n",
        "    if scores==1:\n",
        "        Net_Acc = Net_Acc+1\n",
        "\n",
        "    i = i+1\n",
        "Acc = (Net_Acc/40)*100\n",
        "FN=20-TP\n",
        "FP=20-TN\n",
        "\n",
        "Best_Acc = Acc\n",
        "print(\"Acc ====================================\", Best_Acc)\n",
        "print(\"Sensitivity =\", (TP/(20))*100)\n",
        "print(\"Specificity =\", (TN/(20))*100)\n",
        "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynP1bIkd8DD4"
      },
      "source": [
        "**Regenerating Results of Table V**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuzBs7I58GnS",
        "outputId": "1ca67d1a-693e-4ec1-9d5e-ba3fbbc6d99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
            "X contains 26 input features only and there is no output label (1040, 26)\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Importing the data of Training Database\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq7P_T708NOb",
        "outputId": "9941f62b-9860-4a37-c0e0-285ec6822ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40, 27)\n",
            "(40, 26)\n",
            "(40,)\n",
            "[   0   26   52   78  104  130  156  182  208  234  260  286  312  338\n",
            "  364  390  416  442  468  494  520  546  572  598  624  650  676  702\n",
            "  728  754  780  806  832  858  884  910  936  962  988 1014]\n"
          ]
        }
      ],
      "source": [
        "#Extracting Vowel \"a\" Dataset\n",
        "i=1\n",
        "indices = 0 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
        "increment=26\n",
        "while i<=39:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y = X_train_withlabels[:, 26]\n",
        "X = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGp1-l5n8SLf",
        "outputId": "62a474d1-a052-42c8-998c-706e7b051112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc ==================================== 92.5\n",
            "Sensitivity = 95.0\n",
            "Specificity = 90.0\n",
            "Mathew Correlation Coefficients MCC= 0.8510644963469901\n"
          ]
        }
      ],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_new = lda.fit_transform(X, Y)\n",
        "\n",
        "Best_Acc =0\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "\n",
        "i=0\n",
        "Net_Acc=0\n",
        "while i<=39:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test = X_new[[i]]\n",
        "    X_train = numpy.delete(X_new, i, 0)\n",
        "    Y_train = numpy.delete(Y, i, 0)\n",
        "    Y_test = Y[[i]]\n",
        "\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
        "    model.fit(X_train, Y_train)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    # evaluate the model\n",
        "    scores = accuracy_score(Y_test, Y_pred)\n",
        "    \n",
        "    if i<=19:\n",
        "        if scores==1:\n",
        "            TP = TP+1\n",
        "    if i>19:\n",
        "        if scores==1:\n",
        "            TN = TN+1\n",
        "    \n",
        "\n",
        "    if scores==1:\n",
        "        Net_Acc = Net_Acc+1\n",
        "\n",
        "    i = i+1\n",
        "Acc = (Net_Acc/40)*100\n",
        "FN=20-TP\n",
        "FP=20-TN\n",
        "\n",
        "Best_Acc = Acc\n",
        "print(\"Acc ====================================\", Best_Acc)\n",
        "print(\"Sensitivity =\", (TP/(20))*100)\n",
        "print(\"Specificity =\", (TN/(20))*100)\n",
        "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WlrLJFT9GTz"
      },
      "source": [
        "**Regenerating Results of Table VI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-cqTG_O8_WK",
        "outputId": "43245b37-1cc5-4ca8-d984-59716f37324a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
            "X contains 26 input features only and there is no output label (1040, 26)\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#Importing the Train Data \n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cr3Pzia9V9U",
        "outputId": "b5b11107-b5f4-48e3-a03d-51a74fc1367a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80,)\n",
            "(80, 27)\n",
            "(80, 26)\n",
            "(80,)\n"
          ]
        }
      ],
      "source": [
        "#Extracting vowel data from training database\n",
        "i=2\n",
        "indices = numpy.array(range(0, 2))\n",
        "lb = 26\n",
        "ub = 28\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, range(lb, ub))    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    lb = lb+26\n",
        "    ub = ub+26\n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "#print(indices)\n",
        "X_train_withlabels = X_withlabels[[indices]]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lLwUKsV9eBa",
        "outputId": "b08de155-85c4-4c05-ae8a-9207df8b3d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_with contains 26 input features and class label but not with patient id (168, 27)\n",
            "(168, 26)\n",
            "(168,)\n"
          ]
        }
      ],
      "source": [
        "#Importing the Test Data \n",
        "df = pd.read_csv(\"/content/test_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
        "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
        "print(X_test.shape)\n",
        "Y_test = X_test_withlabels[:, 26]\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTP-E8IN9imc"
      },
      "outputs": [],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
        "\n",
        "\n",
        "######It is important to note below that we are using LDA model fitted on Training Data and Reduce the testing data then.\n",
        "lda = LDA(n_components=1)\n",
        "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agHOlJ4H9rP3",
        "outputId": "3d9633b3-2c09-4900-e263-a25f99e65501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Acc = 100.0\n"
          ]
        }
      ],
      "source": [
        "########### LOSO on Testing Database##############################\n",
        "\n",
        "Best_Acc=0\n",
        "\n",
        "Net_Acc=0\n",
        "i=1\n",
        "lb=0\n",
        "ub=6\n",
        "while i<=28:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test_subj = X_FS_test[range(lb,ub)]\n",
        "    Y_test_subj = Y_test[range(lb,ub)]\n",
        "    #print(X_test_subj.shape)\n",
        "    #print(Y_test_subj.shape)\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    ##############Models Part##############################################\n",
        "\n",
        "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,), random_state=1)\n",
        "    model.fit(X_FS_train, Y_train)\n",
        "    Y_pred = model.predict(X_test_subj)\n",
        "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
        "    if scores>=0.5:\n",
        "        Net_Acc = Net_Acc+1\n",
        "\n",
        "\n",
        "    lb=lb+6\n",
        "    ub=lb+6\n",
        "    i = i+1\n",
        "Acc = (Net_Acc*100)/28\n",
        "\n",
        "Best_Acc = Acc\n",
        "print(\"Best Acc =\", Best_Acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPhD8xwanngUQY1l5Lau53U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}