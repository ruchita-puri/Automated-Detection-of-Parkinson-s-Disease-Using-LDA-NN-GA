{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchita-puri/Automated-Detection-of-Parkinson-s-Disease-Using-LDA-NN-GA/blob/main/FRDB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ZTeEWPv5JDl"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import zscore\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "global Best_Acc\n",
        "Best_Acc=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfz-aNozD3rZ",
        "outputId": "174ac891-dea2-4523-809f-c9823def2601"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cb103d6cd97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Importing the Data of Training Database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/train_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/train_data.csv'"
          ]
        }
      ],
      "source": [
        "#Importing the Data of Training Database\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_train = numpy.delete(X_first, 27, 1)\n",
        "X_withlabels = numpy.delete(X_train, 0,1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
        "X = numpy.delete(X_withlabels, 26, 1)\n",
        "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
        "Y = X_withlabels[: ,26]\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B_-HIVqRECwJ"
      },
      "outputs": [],
      "source": [
        "#Extracting Vowel \"a\" phonations.\n",
        "i=2\n",
        "indices = 0\n",
        "increment = 26\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "    \n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "print(indices)\n",
        "X_train_withlabels = X_withlabels[indices]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJ6RhjPEKje"
      },
      "source": [
        "# Performance on Training Database using Vowel A using LDA-GA-NN. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xJxI-uWdEPmF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import itertools\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "X_Origionl = X_train\n",
        "y_Origionl = Y_train\n",
        "\n",
        "x = range(1,101)\n",
        "lda = LDA(n_components=1)\n",
        "X = lda.fit_transform(X_Origionl, y_Origionl)\n",
        "y = y_Origionl\n",
        "paramgrid = {\"hidden_layer_sizes\": list(itertools.combinations(x,2)),\n",
        "             \"solver\" :['adam']\n",
        "            \n",
        "             }\n",
        "print(\"Size: \", len(paramgrid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C3svVUI4EXWG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#time_a = time.time()\n",
        "if __name__==\"__main__\":\n",
        "    #pool = Pool(4)\n",
        "    cv = EvolutionaryAlgorithmSearchCV(estimator=MLPClassifier(),\n",
        "                                       params=paramgrid,\n",
        "                                       scoring=\"accuracy\",\n",
        "                                       cv=KFold(n_splits=40),\n",
        "                                       verbose=True,\n",
        "                                       population_size=15,\n",
        "                                       gene_mutation_prob=0.10,\n",
        "                                       tournament_size=3,\n",
        "                                       generations_number=10)\n",
        "                                       #pmap = pool.map)\n",
        "    %time cv.fit(X, y)\n",
        "#time_b = time.time()\n",
        "#print(\"Time Elapsed =\", time_b-time_a)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm6silEBEkPB"
      },
      "source": [
        "# Regenerating Results Reported in Paper (Training Database, Vowel A Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OMdeHIG1EdPC"
      },
      "outputs": [],
      "source": [
        "i=2\n",
        "indices = 0\n",
        "increment = 26\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "    \n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "print(indices)\n",
        "X_train_withlabels = X_withlabels[indices]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WbbWw6yPGqua"
      },
      "outputs": [],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_FS = lda.fit_transform(X_train, Y_train)\n",
        "Y_data = Y_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V2TN1UENE0eM"
      },
      "outputs": [],
      "source": [
        "\n",
        "i = 0\n",
        "cor_class=0\n",
        "LOSO_Acc=0\n",
        "while i<=39:\n",
        "    X_test = X_FS[[i]]\n",
        "    X_train = numpy.delete(X_FS, i, 0)\n",
        "    Y_test = Y_data[[i]]\n",
        "    Y_train = numpy.delete(Y_data, i, 0)\n",
        "    ###############For Each Selected Features, evaluate model performance by trying different nodes############\n",
        "    #model =  MLPClassifier(solver='adam', hidden_layer_sizes=(nodes1,  ), random_state=1)\n",
        "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(32, 2, ), random_state=1)\n",
        "    model.fit(X_train, Y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    if(i<=19):\n",
        "        if predictions==1:\n",
        "            cor_class = cor_class+1\n",
        "    if(i>19):\n",
        "        if predictions==0:\n",
        "            cor_class = cor_class+1\n",
        "\n",
        "    i = i+1\n",
        "LOSO_Acc= (cor_class/40)*100\n",
        "if (LOSO_Acc>=80):\n",
        "    Best_Acc = LOSO_Acc\n",
        "    print(\"Best Acc ====================================\", Best_Acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvYoDbjfE4NG"
      },
      "source": [
        "# Regenerating Results Reported in Paper (Testing Database, Vowel A Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eworw2PlE42H"
      },
      "outputs": [],
      "source": [
        "i=2\n",
        "indices = 0\n",
        "increment = 26\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "    \n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "print(indices)\n",
        "X_train_withlabels = X_withlabels[indices]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fDV-V-cSFBZH"
      },
      "outputs": [],
      "source": [
        "#Importing the Test Data \n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/test_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
        "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
        "print(X_test.shape)\n",
        "Y_test = X_test_withlabels[:, 26]\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cQ4NueZzFIrF"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "lda = LDA(n_components=1)\n",
        "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
        "\n",
        "\n",
        "X_test = numpy.delete(X_test, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_test.shape)\n",
        "\n",
        "lda = LDA(n_components=1)\n",
        "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "#lda = LDA(n_components=1)\n",
        "#X_FS_test = lda.fit_transform(X_test, Y_test)\n",
        "#print(X_FS_test_usingtrain-X_FS_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rzW_D3JpFONo"
      },
      "outputs": [],
      "source": [
        "Net_Acc=0\n",
        "i=1\n",
        "lb=0 #Considering Vowel a only\n",
        "ub=26\n",
        "while i<=28:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test_subj = X_FS_test[range(lb,ub)]\n",
        "    Y_test_subj = Y_test[range(lb,ub)]\n",
        "    #print(X_test_subj.shape)\n",
        "    #print(Y_test_subj.shape)\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    ##############Models Part##############################################\n",
        "\n",
        "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(32, 2, ), random_state=1)\n",
        "    model.fit(X_FS_train, Y_train)\n",
        "    Y_pred = model.predict(X_test_subj)\n",
        "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
        "    if scores>0.5:\n",
        "        Net_Acc = Net_Acc+1\n",
        "\n",
        "\n",
        "    lb=lb+6\n",
        "    ub=lb+3\n",
        "    i = i+1\n",
        "Acc = (Net_Acc*100)/28\n",
        "#if (Acc>=60):\n",
        "    #Best_Acc = Acc\n",
        "print(\"Best Acc =\", Acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X6IV-FPFRg6"
      },
      "source": [
        "# Regenerating Results for Vowel O Dataset (Considering Training Database)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "chZ4D0F0FWGi"
      },
      "outputs": [],
      "source": [
        "i=2\n",
        "indices = 1\n",
        "increment = 27\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "    \n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "print(indices)\n",
        "X_train_withlabels = X_withlabels[indices]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JxjNxXdaFbpo"
      },
      "outputs": [],
      "source": [
        "lda = LDA(n_components=1)\n",
        "X_FS = lda.fit_transform(X_train, Y_train)\n",
        "Y_data = Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kYlAzoFwFgV3"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "cor_class=0\n",
        "LOSO_Acc=0\n",
        "while i<=39:\n",
        "    X_test = X_FS[[i]]\n",
        "    X_train = numpy.delete(X_FS, i, 0)\n",
        "    Y_test = Y_data[[i]]\n",
        "    Y_train = numpy.delete(Y_data, i, 0)\n",
        "    ###############For Each Selected Features, evaluate model performance by trying different nodes############\n",
        "    #model =  MLPClassifier(solver='adam', hidden_layer_sizes=(nodes1,  ), random_state=1)\n",
        "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(6, 7, ), random_state=1)\n",
        "    model.fit(X_train, Y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    if(i<=19):\n",
        "        if predictions==1:\n",
        "            cor_class = cor_class+1\n",
        "    if(i>19):\n",
        "        if predictions==0:\n",
        "            cor_class = cor_class+1\n",
        "\n",
        "    i = i+1\n",
        "LOSO_Acc= (cor_class/40)*100\n",
        "if (LOSO_Acc>=80):\n",
        "    Best_Acc = LOSO_Acc\n",
        "    print(\"Best Acc ====================================\", Best_Acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZySLwRrFmcM"
      },
      "source": [
        "# Regenerating Results Reported in Paper (Testing Database, Vowel o Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "plO7qpDFFivo"
      },
      "outputs": [],
      "source": [
        "i=2\n",
        "indices = 1\n",
        "increment = 27\n",
        "while i<=40:\n",
        "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
        "    i = i+1\n",
        "    increment = increment+26\n",
        "    \n",
        "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
        "print(indices)\n",
        "X_train_withlabels = X_withlabels[indices]\n",
        "print(X_train_withlabels.shape)\n",
        "Y_train = X_train_withlabels[:, 26]\n",
        "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IVK3jB-JFxrT"
      },
      "outputs": [],
      "source": [
        "#Importing the Test Data \n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/test_data.csv\")\n",
        "X_first = numpy.array(df)\n",
        "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
        "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
        "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
        "print(X_test.shape)\n",
        "Y_test = X_test_withlabels[:, 26]\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1EZwX8APF3Aj"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "lda = LDA(n_components=1)\n",
        "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
        "\n",
        "\n",
        "X_test = numpy.delete(X_test, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
        "print(X_test.shape)\n",
        "\n",
        "lda = LDA(n_components=1)\n",
        "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6sA49WDuF8-m"
      },
      "outputs": [],
      "source": [
        "Net_Acc=0\n",
        "i=1\n",
        "lb=3   ####This command is different from that of Data a. \n",
        "ub=6\n",
        "while i<=28:\n",
        "\n",
        "    #########Setting Train Test Parts for Each Subject\n",
        "    X_test_subj = X_FS_test[range(lb,ub)]\n",
        "    Y_test_subj = Y_test[range(lb,ub)]\n",
        "    #print(X_test_subj.shape)\n",
        "    #print(Y_test_subj.shape)\n",
        "\n",
        "    ##############Models Part##############################################\n",
        "    #Model Fiting\n",
        "    # checkpoint\n",
        "    ##############Models Part##############################################\n",
        "\n",
        "    #model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(nodes1, nodes2, ), random_state=1)\n",
        "    model =  MLPClassifier(solver='adam', hidden_layer_sizes=(6, 7, ), random_state=1)\n",
        "    model.fit(X_FS_train, Y_train)\n",
        "    Y_pred = model.predict(X_test_subj)\n",
        "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
        "    if scores>0.5:\n",
        "        Net_Acc = Net_Acc+1\n",
        "\n",
        "\n",
        "    lb=lb+6\n",
        "    ub=lb+3\n",
        "    i = i+1\n",
        "Acc = (Net_Acc*100)/28\n",
        "print(\"Best Acc =\", Acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZgXr+vljMFY347o+7HcKc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}